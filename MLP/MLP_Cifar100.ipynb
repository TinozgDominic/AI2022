{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4ZPvUW0QaxC3+ivJWH6OZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TinozgDominic/AI2022/blob/main/MLP/MLP_Cifar100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gz5ABHNJsg9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar100"
      ],
      "metadata": {
        "id": "cbZvK1ShJzwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "\n",
        "print(f'Input shape: {x_train[0].shape}')\n",
        "print(f'Output shape: {y_train[0].shape}')\n",
        "print(f'Train size: {len(x_train)}')\n",
        "print(f'Test size: {len(x_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhadwA4qJ1K7",
        "outputId": "fb8e6d89-1c06-4244-99f2-3c4d05dd980d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (32, 32, 3)\n",
            "Output shape: (1,)\n",
            "Train size: 50000\n",
            "Test size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tf.image.rgb_to_grayscale(x_train)\n",
        "x_test = tf.image.rgb_to_grayscale(x_test)\n",
        "\n",
        "print(f'Input shape: {x_train[0].shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNitP3r5J9Qi",
        "outputId": "2b24a024-20be-452f-d723-407d04436c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (32, 32, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "print(f'Output shape: {y_train[0].shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUWD8FpLJ32s",
        "outputId": "fa379fd9-d6ef-44cd-c6d5-3cfffc70f0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(Flatten(input_shape = [32,32,1]))\n",
        "model.add(layers.Dense(1000, activation = 'relu'))\n",
        "model.add(layers.Dense(1000, activation = 'relu'))\n",
        "model.add(layers.Dense(1000, activation = 'relu'))\n",
        "model.add(layers.Dense(100, activation = 'softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhrYTtbIJ7O0",
        "outputId": "6f5e95b7-7ee4-4f94-897f-7a1715ef023b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_8 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 1000)              1025000   \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 100)               100100    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,127,100\n",
            "Trainable params: 3,127,100\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = 'accuracy')"
      ],
      "metadata": {
        "id": "3glDKmXPLKkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
        "\n",
        "callbacks = [Callback(), \n",
        "            EarlyStopping(patience=21, verbose=1),\n",
        "            ReduceLROnPlateau(patience=5, verbose=1),\n",
        "            ModelCheckpoint('MLP_Cifar100.h5', verbose=1, save_best_only=True)]"
      ],
      "metadata": {
        "id": "fW2OEqyFKJ2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, \n",
        "          epochs = 100, \n",
        "          batch_size = 200,\n",
        "          validation_data = (x_test, y_test), \n",
        "          callbacks = callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvD_t5GqKMh6",
        "outputId": "1c9155a0-887f-42e4-fa58-ee8ca473961e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 21.8585 - accuracy: 0.0108\n",
            "Epoch 1: val_loss improved from inf to 4.59074, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 24s 95ms/step - loss: 21.8585 - accuracy: 0.0108 - val_loss: 4.5907 - val_accuracy: 0.0130 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.5682 - accuracy: 0.0172\n",
            "Epoch 2: val_loss improved from 4.59074 to 4.55232, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 4.5682 - accuracy: 0.0172 - val_loss: 4.5523 - val_accuracy: 0.0218 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.4919 - accuracy: 0.0282\n",
            "Epoch 3: val_loss improved from 4.55232 to 4.47037, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 4.4919 - accuracy: 0.0282 - val_loss: 4.4704 - val_accuracy: 0.0304 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.4225 - accuracy: 0.0361\n",
            "Epoch 4: val_loss improved from 4.47037 to 4.40894, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 4.4225 - accuracy: 0.0361 - val_loss: 4.4089 - val_accuracy: 0.0377 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.3936 - accuracy: 0.0376\n",
            "Epoch 5: val_loss did not improve from 4.40894\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 4.3936 - accuracy: 0.0376 - val_loss: 4.4153 - val_accuracy: 0.0380 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.3475 - accuracy: 0.0435\n",
            "Epoch 6: val_loss improved from 4.40894 to 4.37082, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 24s 96ms/step - loss: 4.3475 - accuracy: 0.0435 - val_loss: 4.3708 - val_accuracy: 0.0410 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.3129 - accuracy: 0.0497\n",
            "Epoch 7: val_loss improved from 4.37082 to 4.36746, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 4.3129 - accuracy: 0.0497 - val_loss: 4.3675 - val_accuracy: 0.0452 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.2890 - accuracy: 0.0550\n",
            "Epoch 8: val_loss improved from 4.36746 to 4.32130, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 4.2890 - accuracy: 0.0550 - val_loss: 4.3213 - val_accuracy: 0.0537 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.2554 - accuracy: 0.0600\n",
            "Epoch 9: val_loss did not improve from 4.32130\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 4.2554 - accuracy: 0.0600 - val_loss: 4.3490 - val_accuracy: 0.0514 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.2411 - accuracy: 0.0619\n",
            "Epoch 10: val_loss improved from 4.32130 to 4.26592, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 23s 93ms/step - loss: 4.2411 - accuracy: 0.0619 - val_loss: 4.2659 - val_accuracy: 0.0625 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.2089 - accuracy: 0.0663\n",
            "Epoch 11: val_loss improved from 4.26592 to 4.22599, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 24s 94ms/step - loss: 4.2089 - accuracy: 0.0663 - val_loss: 4.2260 - val_accuracy: 0.0668 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.1512 - accuracy: 0.0776\n",
            "Epoch 12: val_loss improved from 4.22599 to 4.21780, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 24s 97ms/step - loss: 4.1512 - accuracy: 0.0776 - val_loss: 4.2178 - val_accuracy: 0.0693 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.1360 - accuracy: 0.0797\n",
            "Epoch 13: val_loss did not improve from 4.21780\n",
            "250/250 [==============================] - 25s 99ms/step - loss: 4.1360 - accuracy: 0.0797 - val_loss: 4.2675 - val_accuracy: 0.0600 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.1051 - accuracy: 0.0828\n",
            "Epoch 14: val_loss improved from 4.21780 to 4.16814, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 23s 93ms/step - loss: 4.1051 - accuracy: 0.0828 - val_loss: 4.1681 - val_accuracy: 0.0757 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.0844 - accuracy: 0.0869\n",
            "Epoch 15: val_loss improved from 4.16814 to 4.14256, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 4.0844 - accuracy: 0.0869 - val_loss: 4.1426 - val_accuracy: 0.0858 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.0507 - accuracy: 0.0919\n",
            "Epoch 16: val_loss improved from 4.14256 to 4.12045, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 4.0507 - accuracy: 0.0919 - val_loss: 4.1205 - val_accuracy: 0.0841 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 4.0089 - accuracy: 0.0970\n",
            "Epoch 17: val_loss improved from 4.12045 to 4.09651, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 23s 93ms/step - loss: 4.0089 - accuracy: 0.0970 - val_loss: 4.0965 - val_accuracy: 0.0960 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.9799 - accuracy: 0.1032\n",
            "Epoch 18: val_loss did not improve from 4.09651\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 3.9799 - accuracy: 0.1032 - val_loss: 4.1099 - val_accuracy: 0.0946 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.9537 - accuracy: 0.1084\n",
            "Epoch 19: val_loss improved from 4.09651 to 4.09255, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 23s 93ms/step - loss: 3.9537 - accuracy: 0.1084 - val_loss: 4.0925 - val_accuracy: 0.0950 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.9329 - accuracy: 0.1086\n",
            "Epoch 20: val_loss improved from 4.09255 to 4.04487, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 24s 95ms/step - loss: 3.9329 - accuracy: 0.1086 - val_loss: 4.0449 - val_accuracy: 0.1018 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.8938 - accuracy: 0.1161\n",
            "Epoch 21: val_loss did not improve from 4.04487\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 3.8938 - accuracy: 0.1161 - val_loss: 4.0891 - val_accuracy: 0.0974 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.8952 - accuracy: 0.1168\n",
            "Epoch 22: val_loss did not improve from 4.04487\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 3.8952 - accuracy: 0.1168 - val_loss: 4.0658 - val_accuracy: 0.1030 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.8578 - accuracy: 0.1232\n",
            "Epoch 23: val_loss did not improve from 4.04487\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 3.8578 - accuracy: 0.1232 - val_loss: 4.0790 - val_accuracy: 0.0968 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.8401 - accuracy: 0.1235\n",
            "Epoch 24: val_loss did not improve from 4.04487\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 3.8401 - accuracy: 0.1235 - val_loss: 4.0756 - val_accuracy: 0.0994 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.8036 - accuracy: 0.1311\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 25: val_loss did not improve from 4.04487\n",
            "250/250 [==============================] - 23s 90ms/step - loss: 3.8036 - accuracy: 0.1311 - val_loss: 4.1185 - val_accuracy: 0.1041 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.6143 - accuracy: 0.1590\n",
            "Epoch 26: val_loss improved from 4.04487 to 4.01610, saving model to MLP_Cifar100.h5\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 3.6143 - accuracy: 0.1590 - val_loss: 4.0161 - val_accuracy: 0.1220 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.5427 - accuracy: 0.1729\n",
            "Epoch 27: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 28s 113ms/step - loss: 3.5427 - accuracy: 0.1729 - val_loss: 4.0316 - val_accuracy: 0.1214 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.5036 - accuracy: 0.1800\n",
            "Epoch 28: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 3.5036 - accuracy: 0.1800 - val_loss: 4.0476 - val_accuracy: 0.1238 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.4723 - accuracy: 0.1873\n",
            "Epoch 29: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 3.4723 - accuracy: 0.1873 - val_loss: 4.0608 - val_accuracy: 0.1253 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.4438 - accuracy: 0.1922\n",
            "Epoch 30: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 90ms/step - loss: 3.4438 - accuracy: 0.1922 - val_loss: 4.0694 - val_accuracy: 0.1252 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.4154 - accuracy: 0.1958\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 31: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 22s 90ms/step - loss: 3.4154 - accuracy: 0.1958 - val_loss: 4.0896 - val_accuracy: 0.1240 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3600 - accuracy: 0.2084\n",
            "Epoch 32: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 3.3600 - accuracy: 0.2084 - val_loss: 4.0893 - val_accuracy: 0.1254 - lr: 1.0000e-05\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3533 - accuracy: 0.2103\n",
            "Epoch 33: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 3.3533 - accuracy: 0.2103 - val_loss: 4.0890 - val_accuracy: 0.1263 - lr: 1.0000e-05\n",
            "Epoch 34/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3490 - accuracy: 0.2104\n",
            "Epoch 34: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 22s 90ms/step - loss: 3.3490 - accuracy: 0.2104 - val_loss: 4.0949 - val_accuracy: 0.1261 - lr: 1.0000e-05\n",
            "Epoch 35/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3450 - accuracy: 0.2114\n",
            "Epoch 35: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 24s 95ms/step - loss: 3.3450 - accuracy: 0.2114 - val_loss: 4.0981 - val_accuracy: 0.1258 - lr: 1.0000e-05\n",
            "Epoch 36/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3411 - accuracy: 0.2123\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "\n",
            "Epoch 36: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 3.3411 - accuracy: 0.2123 - val_loss: 4.0998 - val_accuracy: 0.1255 - lr: 1.0000e-05\n",
            "Epoch 37/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3343 - accuracy: 0.2138\n",
            "Epoch 37: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 3.3343 - accuracy: 0.2138 - val_loss: 4.0994 - val_accuracy: 0.1258 - lr: 1.0000e-06\n",
            "Epoch 38/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3336 - accuracy: 0.2140\n",
            "Epoch 38: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 93ms/step - loss: 3.3336 - accuracy: 0.2140 - val_loss: 4.0998 - val_accuracy: 0.1261 - lr: 1.0000e-06\n",
            "Epoch 39/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3331 - accuracy: 0.2140\n",
            "Epoch 39: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 3.3331 - accuracy: 0.2140 - val_loss: 4.1000 - val_accuracy: 0.1260 - lr: 1.0000e-06\n",
            "Epoch 40/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3327 - accuracy: 0.2142\n",
            "Epoch 40: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 3.3327 - accuracy: 0.2142 - val_loss: 4.1003 - val_accuracy: 0.1258 - lr: 1.0000e-06\n",
            "Epoch 41/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3323 - accuracy: 0.2143\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "\n",
            "Epoch 41: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 3.3323 - accuracy: 0.2143 - val_loss: 4.1002 - val_accuracy: 0.1260 - lr: 1.0000e-06\n",
            "Epoch 42/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3315 - accuracy: 0.2146\n",
            "Epoch 42: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 24s 95ms/step - loss: 3.3315 - accuracy: 0.2146 - val_loss: 4.1004 - val_accuracy: 0.1260 - lr: 1.0000e-07\n",
            "Epoch 43/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3315 - accuracy: 0.2145\n",
            "Epoch 43: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 92ms/step - loss: 3.3315 - accuracy: 0.2145 - val_loss: 4.1005 - val_accuracy: 0.1261 - lr: 1.0000e-07\n",
            "Epoch 44/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3314 - accuracy: 0.2146\n",
            "Epoch 44: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 90ms/step - loss: 3.3314 - accuracy: 0.2146 - val_loss: 4.1006 - val_accuracy: 0.1261 - lr: 1.0000e-07\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3314 - accuracy: 0.2146\n",
            "Epoch 45: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 91ms/step - loss: 3.3314 - accuracy: 0.2146 - val_loss: 4.1007 - val_accuracy: 0.1261 - lr: 1.0000e-07\n",
            "Epoch 46/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3313 - accuracy: 0.2145\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "\n",
            "Epoch 46: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 23s 90ms/step - loss: 3.3313 - accuracy: 0.2145 - val_loss: 4.1007 - val_accuracy: 0.1261 - lr: 1.0000e-07\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - ETA: 0s - loss: 3.3312 - accuracy: 0.2145\n",
            "Epoch 47: val_loss did not improve from 4.01610\n",
            "250/250 [==============================] - 22s 89ms/step - loss: 3.3312 - accuracy: 0.2145 - val_loss: 4.1007 - val_accuracy: 0.1261 - lr: 1.0000e-08\n",
            "Epoch 47: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f41d1e4fb10>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "print('Predict:', np.argmax(model.predict(x_test[show])))\n",
        "print('True mask:' ,np.argmax(y_test[show]))\n",
        "\n",
        "plt.imshow(tf.reshape(x_test[show],[32,32]))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j2wUCUSfV7qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show = random.randint(0,9999)\n",
        "\n",
        "print('Predict:', np.argmax(model.predict(x_test[show])))\n",
        "print('True mask:' ,np.argmax(y_test[show]))\n",
        "\n",
        "plt.imshow(tf.reshape(x_test[show],[32,32]))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "QIWXPAisZmvZ",
        "outputId": "e54259a3-1fec-4f51-b691-ddbd46896912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 57ms/step\n",
            "Predict: 44\n",
            "True mask: 57\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}