{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0n0vsoBoPkuSB/Q++tH7a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TinozgDominic/AI2022/blob/main/MLP/MLP_Cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "kJIh_OHbS35Y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10"
      ],
      "metadata": {
        "id": "z0DXpSQI7g9E"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(f'Input shape: {x_train[0].shape}')\n",
        "print(f'Output shape: {y_train[0].shape}')\n",
        "print(f'Train size: {len(x_train)}')\n",
        "print(f'Test size: {len(x_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1rRulaC75Qj",
        "outputId": "cddb7e41-b0b7-42c0-9ea2-6067edb430b6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (32, 32, 3)\n",
            "Output shape: (1,)\n",
            "Train size: 50000\n",
            "Test size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tf.image.rgb_to_grayscale(x_train)\n",
        "x_test = tf.image.rgb_to_grayscale(x_test)\n",
        "\n",
        "print(f'Input shape: {x_train[0].shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RiZfnsh-p8c",
        "outputId": "ba4cc077-2689-4fd0-b8b3-eabd1e133192"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (32, 32, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "vL1Da0Wh8owP"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Output shape: {y_train[0].shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smC1Yz1b9MX0",
        "outputId": "bd8d5aa5-5677-414f-833c-a01b6ded9553"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(Flatten(input_shape = [32,32,1]))\n",
        "model.add(layers.Dense(300, activation = 'relu'))\n",
        "model.add(layers.Dense(100, activation = 'relu'))\n",
        "model.add(layers.Dense(10, activation = 'softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = 'accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IOwl-Jb8Bfm",
        "outputId": "7b339949-fc1a-4090-aa80-cdd3406c17b7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_4 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 300)               307500    \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 100)               30100     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 338,610\n",
            "Trainable params: 338,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
        "\n",
        "callbacks = [Callback(), \n",
        "            EarlyStopping(patience=21, verbose=1),\n",
        "            ReduceLROnPlateau(patience=5, verbose=1),\n",
        "            ModelCheckpoint('MLP_Cifar10.h5', verbose=1, save_best_only=True)]"
      ],
      "metadata": {
        "id": "k_ngdwmM8TAy"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs = 100, batch_size = 100,validation_data=(x_test, y_test), callbacks = callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goOSMFxu8cY6",
        "outputId": "46d80f30-78aa-4618-f390-0db098907b86"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 17.6635 - accuracy: 0.1571\n",
            "Epoch 1: val_loss improved from inf to 5.31997, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 9s 16ms/step - loss: 17.6375 - accuracy: 0.1570 - val_loss: 5.3200 - val_accuracy: 0.1450 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 4.1812 - accuracy: 0.1715\n",
            "Epoch 2: val_loss improved from 5.31997 to 3.47459, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 9s 17ms/step - loss: 4.1769 - accuracy: 0.1715 - val_loss: 3.4746 - val_accuracy: 0.1603 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 2.7600 - accuracy: 0.1962\n",
            "Epoch 3: val_loss improved from 3.47459 to 2.44514, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 7s 14ms/step - loss: 2.7590 - accuracy: 0.1961 - val_loss: 2.4451 - val_accuracy: 0.2065 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 2.4099 - accuracy: 0.2119\n",
            "Epoch 4: val_loss improved from 2.44514 to 2.24436, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.4100 - accuracy: 0.2118 - val_loss: 2.2444 - val_accuracy: 0.2388 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 2.2159 - accuracy: 0.2360\n",
            "Epoch 5: val_loss did not improve from 2.24436\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.2152 - accuracy: 0.2362 - val_loss: 2.3394 - val_accuracy: 0.2184 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 2.1540 - accuracy: 0.2440\n",
            "Epoch 6: val_loss improved from 2.24436 to 2.14329, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.1540 - accuracy: 0.2440 - val_loss: 2.1433 - val_accuracy: 0.2487 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 2.0936 - accuracy: 0.2578\n",
            "Epoch 7: val_loss improved from 2.14329 to 2.09392, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 2.0938 - accuracy: 0.2579 - val_loss: 2.0939 - val_accuracy: 0.2589 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 2.0594 - accuracy: 0.2677\n",
            "Epoch 8: val_loss improved from 2.09392 to 2.04319, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 2.0591 - accuracy: 0.2678 - val_loss: 2.0432 - val_accuracy: 0.2754 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 2.0332 - accuracy: 0.2765\n",
            "Epoch 9: val_loss did not improve from 2.04319\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 2.0332 - accuracy: 0.2766 - val_loss: 2.0891 - val_accuracy: 0.2606 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 2.0093 - accuracy: 0.2846\n",
            "Epoch 10: val_loss improved from 2.04319 to 2.02989, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 2.0089 - accuracy: 0.2847 - val_loss: 2.0299 - val_accuracy: 0.2673 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "500/500 [==============================] - ETA: 0s - loss: 2.0040 - accuracy: 0.2848\n",
            "Epoch 11: val_loss improved from 2.02989 to 1.97469, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 2.0040 - accuracy: 0.2848 - val_loss: 1.9747 - val_accuracy: 0.2971 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 1.9756 - accuracy: 0.2917\n",
            "Epoch 12: val_loss did not improve from 1.97469\n",
            "500/500 [==============================] - 5s 11ms/step - loss: 1.9763 - accuracy: 0.2916 - val_loss: 2.0435 - val_accuracy: 0.2879 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 1.9784 - accuracy: 0.2941\n",
            "Epoch 13: val_loss did not improve from 1.97469\n",
            "500/500 [==============================] - 6s 11ms/step - loss: 1.9784 - accuracy: 0.2941 - val_loss: 2.0294 - val_accuracy: 0.2847 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "495/500 [============================>.] - ETA: 0s - loss: 1.9559 - accuracy: 0.2993\n",
            "Epoch 14: val_loss did not improve from 1.97469\n",
            "500/500 [==============================] - 6s 11ms/step - loss: 1.9558 - accuracy: 0.2994 - val_loss: 2.0103 - val_accuracy: 0.2767 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.9631 - accuracy: 0.2980\n",
            "Epoch 15: val_loss did not improve from 1.97469\n",
            "500/500 [==============================] - 6s 11ms/step - loss: 1.9629 - accuracy: 0.2981 - val_loss: 1.9888 - val_accuracy: 0.2878 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 1.9526 - accuracy: 0.2957\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 16: val_loss did not improve from 1.97469\n",
            "500/500 [==============================] - 6s 12ms/step - loss: 1.9528 - accuracy: 0.2956 - val_loss: 1.9976 - val_accuracy: 0.2702 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.8429 - accuracy: 0.3474\n",
            "Epoch 17: val_loss improved from 1.97469 to 1.86047, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 9s 17ms/step - loss: 1.8427 - accuracy: 0.3476 - val_loss: 1.8605 - val_accuracy: 0.3405 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 1.8227 - accuracy: 0.3556\n",
            "Epoch 18: val_loss improved from 1.86047 to 1.85604, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 6s 11ms/step - loss: 1.8225 - accuracy: 0.3556 - val_loss: 1.8560 - val_accuracy: 0.3438 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 1.8177 - accuracy: 0.3536\n",
            "Epoch 19: val_loss did not improve from 1.85604\n",
            "500/500 [==============================] - 5s 11ms/step - loss: 1.8179 - accuracy: 0.3537 - val_loss: 1.8566 - val_accuracy: 0.3383 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 1.8110 - accuracy: 0.3577\n",
            "Epoch 20: val_loss did not improve from 1.85604\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.8107 - accuracy: 0.3579 - val_loss: 1.8609 - val_accuracy: 0.3396 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 1.8067 - accuracy: 0.3617\n",
            "Epoch 21: val_loss improved from 1.85604 to 1.85277, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.8061 - accuracy: 0.3619 - val_loss: 1.8528 - val_accuracy: 0.3404 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            "500/500 [==============================] - ETA: 0s - loss: 1.8010 - accuracy: 0.3612\n",
            "Epoch 22: val_loss improved from 1.85277 to 1.84495, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 11ms/step - loss: 1.8010 - accuracy: 0.3612 - val_loss: 1.8450 - val_accuracy: 0.3466 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 1.7963 - accuracy: 0.3631\n",
            "Epoch 23: val_loss did not improve from 1.84495\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7963 - accuracy: 0.3633 - val_loss: 1.8556 - val_accuracy: 0.3413 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 1.7866 - accuracy: 0.3675\n",
            "Epoch 24: val_loss did not improve from 1.84495\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7864 - accuracy: 0.3675 - val_loss: 1.8473 - val_accuracy: 0.3405 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 1.7808 - accuracy: 0.3678\n",
            "Epoch 25: val_loss improved from 1.84495 to 1.82913, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 11ms/step - loss: 1.7814 - accuracy: 0.3677 - val_loss: 1.8291 - val_accuracy: 0.3544 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 1.7731 - accuracy: 0.3726\n",
            "Epoch 26: val_loss did not improve from 1.82913\n",
            "500/500 [==============================] - 5s 11ms/step - loss: 1.7733 - accuracy: 0.3726 - val_loss: 1.8315 - val_accuracy: 0.3513 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 1.7659 - accuracy: 0.3732\n",
            "Epoch 27: val_loss did not improve from 1.82913\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7663 - accuracy: 0.3730 - val_loss: 1.8369 - val_accuracy: 0.3526 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.7600 - accuracy: 0.3766\n",
            "Epoch 28: val_loss did not improve from 1.82913\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7603 - accuracy: 0.3766 - val_loss: 1.8366 - val_accuracy: 0.3452 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.7547 - accuracy: 0.3786\n",
            "Epoch 29: val_loss improved from 1.82913 to 1.81495, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7546 - accuracy: 0.3787 - val_loss: 1.8150 - val_accuracy: 0.3582 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 1.7492 - accuracy: 0.3814\n",
            "Epoch 30: val_loss did not improve from 1.81495\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7492 - accuracy: 0.3815 - val_loss: 1.8220 - val_accuracy: 0.3566 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.7437 - accuracy: 0.3821\n",
            "Epoch 31: val_loss did not improve from 1.81495\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7436 - accuracy: 0.3822 - val_loss: 1.8155 - val_accuracy: 0.3590 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "500/500 [==============================] - ETA: 0s - loss: 1.7389 - accuracy: 0.3854\n",
            "Epoch 32: val_loss did not improve from 1.81495\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7389 - accuracy: 0.3854 - val_loss: 1.8301 - val_accuracy: 0.3520 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "495/500 [============================>.] - ETA: 0s - loss: 1.7348 - accuracy: 0.3851\n",
            "Epoch 33: val_loss improved from 1.81495 to 1.80753, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 6s 11ms/step - loss: 1.7350 - accuracy: 0.3852 - val_loss: 1.8075 - val_accuracy: 0.3613 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "500/500 [==============================] - ETA: 0s - loss: 1.7303 - accuracy: 0.3901\n",
            "Epoch 34: val_loss did not improve from 1.80753\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7303 - accuracy: 0.3901 - val_loss: 1.8206 - val_accuracy: 0.3617 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 1.7261 - accuracy: 0.3880\n",
            "Epoch 35: val_loss did not improve from 1.80753\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7264 - accuracy: 0.3879 - val_loss: 1.8226 - val_accuracy: 0.3589 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "495/500 [============================>.] - ETA: 0s - loss: 1.7218 - accuracy: 0.3917\n",
            "Epoch 36: val_loss improved from 1.80753 to 1.80454, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7222 - accuracy: 0.3914 - val_loss: 1.8045 - val_accuracy: 0.3613 - lr: 1.0000e-04\n",
            "Epoch 37/100\n",
            "495/500 [============================>.] - ETA: 0s - loss: 1.7178 - accuracy: 0.3931\n",
            "Epoch 37: val_loss did not improve from 1.80454\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7179 - accuracy: 0.3930 - val_loss: 1.8102 - val_accuracy: 0.3629 - lr: 1.0000e-04\n",
            "Epoch 38/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 1.7146 - accuracy: 0.3962\n",
            "Epoch 38: val_loss did not improve from 1.80454\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7144 - accuracy: 0.3961 - val_loss: 1.8137 - val_accuracy: 0.3551 - lr: 1.0000e-04\n",
            "Epoch 39/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 1.7084 - accuracy: 0.3973\n",
            "Epoch 39: val_loss improved from 1.80454 to 1.79649, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7088 - accuracy: 0.3971 - val_loss: 1.7965 - val_accuracy: 0.3691 - lr: 1.0000e-04\n",
            "Epoch 40/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 1.7062 - accuracy: 0.3961\n",
            "Epoch 40: val_loss did not improve from 1.79649\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.7061 - accuracy: 0.3962 - val_loss: 1.8012 - val_accuracy: 0.3661 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 1.7001 - accuracy: 0.3984\n",
            "Epoch 41: val_loss did not improve from 1.79649\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.7006 - accuracy: 0.3981 - val_loss: 1.8103 - val_accuracy: 0.3597 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "500/500 [==============================] - ETA: 0s - loss: 1.6950 - accuracy: 0.4014\n",
            "Epoch 42: val_loss did not improve from 1.79649\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6950 - accuracy: 0.4014 - val_loss: 1.7972 - val_accuracy: 0.3675 - lr: 1.0000e-04\n",
            "Epoch 43/100\n",
            "500/500 [==============================] - ETA: 0s - loss: 1.6911 - accuracy: 0.4027\n",
            "Epoch 43: val_loss improved from 1.79649 to 1.78877, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6911 - accuracy: 0.4027 - val_loss: 1.7888 - val_accuracy: 0.3724 - lr: 1.0000e-04\n",
            "Epoch 44/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 1.6864 - accuracy: 0.4045\n",
            "Epoch 44: val_loss did not improve from 1.78877\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.6865 - accuracy: 0.4044 - val_loss: 1.7974 - val_accuracy: 0.3693 - lr: 1.0000e-04\n",
            "Epoch 45/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.6818 - accuracy: 0.4063\n",
            "Epoch 45: val_loss did not improve from 1.78877\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.6817 - accuracy: 0.4064 - val_loss: 1.8097 - val_accuracy: 0.3638 - lr: 1.0000e-04\n",
            "Epoch 46/100\n",
            "495/500 [============================>.] - ETA: 0s - loss: 1.6756 - accuracy: 0.4084\n",
            "Epoch 46: val_loss did not improve from 1.78877\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6759 - accuracy: 0.4085 - val_loss: 1.7970 - val_accuracy: 0.3701 - lr: 1.0000e-04\n",
            "Epoch 47/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.6731 - accuracy: 0.4104\n",
            "Epoch 47: val_loss did not improve from 1.78877\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.6731 - accuracy: 0.4104 - val_loss: 1.7959 - val_accuracy: 0.3718 - lr: 1.0000e-04\n",
            "Epoch 48/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.6713 - accuracy: 0.4084\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 48: val_loss did not improve from 1.78877\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.6710 - accuracy: 0.4085 - val_loss: 1.7994 - val_accuracy: 0.3693 - lr: 1.0000e-04\n",
            "Epoch 49/100\n",
            "500/500 [==============================] - ETA: 0s - loss: 1.6396 - accuracy: 0.4227\n",
            "Epoch 49: val_loss improved from 1.78877 to 1.77976, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6396 - accuracy: 0.4227 - val_loss: 1.7798 - val_accuracy: 0.3779 - lr: 1.0000e-05\n",
            "Epoch 50/100\n",
            "495/500 [============================>.] - ETA: 0s - loss: 1.6349 - accuracy: 0.4253\n",
            "Epoch 50: val_loss improved from 1.77976 to 1.77955, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.6357 - accuracy: 0.4251 - val_loss: 1.7795 - val_accuracy: 0.3763 - lr: 1.0000e-05\n",
            "Epoch 51/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.6344 - accuracy: 0.4257\n",
            "Epoch 51: val_loss did not improve from 1.77955\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.6345 - accuracy: 0.4256 - val_loss: 1.7810 - val_accuracy: 0.3757 - lr: 1.0000e-05\n",
            "Epoch 52/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 1.6342 - accuracy: 0.4262\n",
            "Epoch 52: val_loss improved from 1.77955 to 1.77931, saving model to MLP_Cifar10.h5\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6339 - accuracy: 0.4264 - val_loss: 1.7793 - val_accuracy: 0.3775 - lr: 1.0000e-05\n",
            "Epoch 53/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 1.6333 - accuracy: 0.4251\n",
            "Epoch 53: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6334 - accuracy: 0.4249 - val_loss: 1.7826 - val_accuracy: 0.3769 - lr: 1.0000e-05\n",
            "Epoch 54/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.6324 - accuracy: 0.4255\n",
            "Epoch 54: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6322 - accuracy: 0.4257 - val_loss: 1.7828 - val_accuracy: 0.3761 - lr: 1.0000e-05\n",
            "Epoch 55/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.6317 - accuracy: 0.4269\n",
            "Epoch 55: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.6317 - accuracy: 0.4269 - val_loss: 1.7816 - val_accuracy: 0.3740 - lr: 1.0000e-05\n",
            "Epoch 56/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.6309 - accuracy: 0.4268\n",
            "Epoch 56: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6309 - accuracy: 0.4267 - val_loss: 1.7849 - val_accuracy: 0.3761 - lr: 1.0000e-05\n",
            "Epoch 57/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 1.6301 - accuracy: 0.4268\n",
            "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "\n",
            "Epoch 57: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.6303 - accuracy: 0.4268 - val_loss: 1.7838 - val_accuracy: 0.3754 - lr: 1.0000e-05\n",
            "Epoch 58/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 1.6268 - accuracy: 0.4289\n",
            "Epoch 58: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6269 - accuracy: 0.4287 - val_loss: 1.7812 - val_accuracy: 0.3774 - lr: 1.0000e-06\n",
            "Epoch 59/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 1.6261 - accuracy: 0.4294\n",
            "Epoch 59: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.6263 - accuracy: 0.4293 - val_loss: 1.7809 - val_accuracy: 0.3771 - lr: 1.0000e-06\n",
            "Epoch 60/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 1.6263 - accuracy: 0.4292\n",
            "Epoch 60: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6260 - accuracy: 0.4293 - val_loss: 1.7809 - val_accuracy: 0.3770 - lr: 1.0000e-06\n",
            "Epoch 61/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 1.6261 - accuracy: 0.4289\n",
            "Epoch 61: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6260 - accuracy: 0.4290 - val_loss: 1.7811 - val_accuracy: 0.3775 - lr: 1.0000e-06\n",
            "Epoch 62/100\n",
            "500/500 [==============================] - ETA: 0s - loss: 1.6259 - accuracy: 0.4299\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "\n",
            "Epoch 62: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6259 - accuracy: 0.4299 - val_loss: 1.7811 - val_accuracy: 0.3772 - lr: 1.0000e-06\n",
            "Epoch 63/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.6251 - accuracy: 0.4293\n",
            "Epoch 63: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6254 - accuracy: 0.4292 - val_loss: 1.7811 - val_accuracy: 0.3777 - lr: 1.0000e-07\n",
            "Epoch 64/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.6255 - accuracy: 0.4294\n",
            "Epoch 64: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6254 - accuracy: 0.4294 - val_loss: 1.7811 - val_accuracy: 0.3778 - lr: 1.0000e-07\n",
            "Epoch 65/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 1.6259 - accuracy: 0.4293\n",
            "Epoch 65: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.6254 - accuracy: 0.4295 - val_loss: 1.7811 - val_accuracy: 0.3777 - lr: 1.0000e-07\n",
            "Epoch 66/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 1.6257 - accuracy: 0.4292\n",
            "Epoch 66: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 6s 11ms/step - loss: 1.6253 - accuracy: 0.4293 - val_loss: 1.7811 - val_accuracy: 0.3775 - lr: 1.0000e-07\n",
            "Epoch 67/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.6253 - accuracy: 0.4295\n",
            "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "\n",
            "Epoch 67: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6253 - accuracy: 0.4296 - val_loss: 1.7811 - val_accuracy: 0.3773 - lr: 1.0000e-07\n",
            "Epoch 68/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 1.6250 - accuracy: 0.4298\n",
            "Epoch 68: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.6253 - accuracy: 0.4298 - val_loss: 1.7811 - val_accuracy: 0.3773 - lr: 1.0000e-08\n",
            "Epoch 69/100\n",
            "496/500 [============================>.] - ETA: 0s - loss: 1.6250 - accuracy: 0.4296\n",
            "Epoch 69: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.6253 - accuracy: 0.4297 - val_loss: 1.7811 - val_accuracy: 0.3773 - lr: 1.0000e-08\n",
            "Epoch 70/100\n",
            "498/500 [============================>.] - ETA: 0s - loss: 1.6254 - accuracy: 0.4299\n",
            "Epoch 70: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6253 - accuracy: 0.4298 - val_loss: 1.7811 - val_accuracy: 0.3775 - lr: 1.0000e-08\n",
            "Epoch 71/100\n",
            "494/500 [============================>.] - ETA: 0s - loss: 1.6247 - accuracy: 0.4301\n",
            "Epoch 71: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 1.6253 - accuracy: 0.4297 - val_loss: 1.7811 - val_accuracy: 0.3775 - lr: 1.0000e-08\n",
            "Epoch 72/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 1.6255 - accuracy: 0.4296\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
            "\n",
            "Epoch 72: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 1.6253 - accuracy: 0.4297 - val_loss: 1.7811 - val_accuracy: 0.3774 - lr: 1.0000e-08\n",
            "Epoch 73/100\n",
            "497/500 [============================>.] - ETA: 0s - loss: 1.6252 - accuracy: 0.4301\n",
            "Epoch 73: val_loss did not improve from 1.77931\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.6253 - accuracy: 0.4298 - val_loss: 1.7811 - val_accuracy: 0.3774 - lr: 1.0000e-09\n",
            "Epoch 73: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5924d12d90>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', \n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "OJJHPEq-EtWf"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "onSZiPxMBJ_C"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show = random.randint(0,9999)\n",
        "\n",
        "plt.imshow(tf.reshape(x_test[show],[32,32]))\n",
        "print('Predict:',classes[np.argmax(model.predict(x_test[show]), axis = 1)[0]])\n",
        "print('True mask:',classes[np.argmax(y_test[show])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "vi0M6BGoIIWw",
        "outputId": "170e3e6c-012d-4d29-edf7-7bb480fa5f3d"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predict: ship\n",
            "True mask: ship\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}